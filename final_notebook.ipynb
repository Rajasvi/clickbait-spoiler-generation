{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install wandb\n",
    "# !pip install datasets\n",
    "# !pip install bert_score\n",
    "# !pip install evaluate\n",
    "# !pip install accelerate\n",
    "# !pip install gradio --upgrade\n",
    "# !pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, RobertaTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import evaluate\n",
    "from transformers import default_data_collator\n",
    "from transformers import Trainer\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import TrainingArguments\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "import gradio as gr\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device (turn on GPU acceleration for faster execution)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorUuid = {\"ad9271b7-9983-42f5-9bd9-fdfcb171ddaa\":[[[4, 37],[4, 222]]]}\n",
    "def parse_spoiler(x):\n",
    "    spoiler = []\n",
    "    if x['uuid'] in errorUuid:\n",
    "        x['spoilerPositions'] = errorUuid[x['uuid']]\n",
    "\n",
    "    for s in x['spoilerPositions']:\n",
    "        st,en = s[0],s[1]\n",
    "        spoiler.append(x['targetParagraphs'][st[0]][st[1]:en[1]])\n",
    "        \n",
    "    return spoiler\n",
    "\n",
    "def findPosTags(x):    \n",
    "    tokPos = []\n",
    "    for pos in x['spoilerPositions']:\n",
    "        st,en = pos\n",
    "        idx = 0\n",
    "        for i,p in enumerate([x['targetTitle']] + x['targetParagraphs']):\n",
    "            if i==st[0]+1:\n",
    "                start_ind = idx+st[1]\n",
    "                end_ind = idx + en[1]\n",
    "                \n",
    "                tokPos.append([start_ind,end_ind])\n",
    "                break\n",
    "            if i==0:\n",
    "                idx+=len(p)+3\n",
    "            else:\n",
    "                idx+=len(p)+1\n",
    "        \n",
    "    return tokPos\n",
    "\n",
    "def read_prep(path,train=True):\n",
    "    with open(path, 'rb') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    results = []\n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        results.append(result)\n",
    "    df = pd.DataFrame(results)\n",
    "    df['tags'] = df.tags.apply(lambda x:x[0],1)\n",
    "    df['postText'] = df.postText.apply(lambda x:x[0],1)    \n",
    "    \n",
    "    # Parsing for faulty spoiler ids\n",
    "    df['spoilerParsed'] = df.apply(parse_spoiler,1)\n",
    "    df['mergedParas'] = df['targetParagraphs'].apply(lambda x:\" \".join(x),1)\n",
    "    df.mergedParas = df.targetTitle + \" - \" + df.mergedParas\n",
    "    df['tokPos'] = df.apply(findPosTags,1)\n",
    "    df['label'] = df['tags'].map({\"phrase\":0,\"passage\":1,\"multi\":2})\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_prep(\"./data/train.jsonl\")\n",
    "df_valid = read_prep(\"./data/validation.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Spoiler Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=5,\n",
    "    classes=3,\n",
    "    batch_size=4,\n",
    "    learning_rate=1e-5,\n",
    "    model=\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2766b0d9674dba98a2f225fa69ec1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd128c9979a4b869960792d6fcb88cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3263659b3894d5383822bd9e1d107fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0f47f2a072427ca7eb3e0448f00468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192647f9f20a4b9d8bb57220041cc914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model'], do_lower_case=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(config['model'],num_labels=config['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    return tokenizer(examples['postText'],\n",
    "                    examples['mergedParas'],\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    padding=True,\n",
    "                    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_pandas(df_train[['postText',\"mergedParas\",\"label\"]])\n",
    "val_data = Dataset.from_pandas(df_valid[['postText',\"mergedParas\",\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0491d8ea93463fba8a9bb304e7a291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_data.map(tokenize,batched=True,remove_columns=[\"postText\",\"mergedParas\"])\n",
    "train_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00d086c7eec4163a027c529be36143b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_dataset = val_data.map(tokenize,batched=True,remove_columns=[\"postText\",\"mergedParas\"])\n",
    "val_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds = np.argmax(eval_preds.predictions,1)\n",
    "    \n",
    "    return {\"accuracy\":(preds == eval_preds.label_ids).astype(np.float32).mean().item() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rvsharma/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4000\n",
      "  Number of trainable parameters = 124647939\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajasvi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rvsharma/lign167/project/wandb/run-20221204_184130-3ookuw3d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rajasvi/huggingface/runs/3ookuw3d\" target=\"_blank\">./trainer_task1_bert</a></strong> to <a href=\"https://wandb.ai/rajasvi/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 12:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.030300</td>\n",
       "      <td>0.862932</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.684117</td>\n",
       "      <td>0.731250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.706200</td>\n",
       "      <td>0.788917</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.597800</td>\n",
       "      <td>1.042842</td>\n",
       "      <td>0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.508300</td>\n",
       "      <td>1.156266</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 800\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 800\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 800\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 800\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 800\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4000, training_loss=0.711951374053955, metrics={'train_runtime': 746.6664, 'train_samples_per_second': 21.429, 'train_steps_per_second': 5.357, 'total_flos': 4209814683648000.0, 'train_loss': 0.711951374053955, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trainer_task1_bert\",\n",
    "    learning_rate=config['learning_rate'],\n",
    "    per_device_train_batch_size=config['batch_size'],\n",
    "    per_device_eval_batch_size=config['batch_size'],\n",
    "    num_train_epochs=config['epochs'],\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy =\"epoch\",\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models_task1/\n",
      "Configuration saved in ./models_task1/config.json\n",
      "Model weights saved in ./models_task1/pytorch_model.bin\n",
      "tokenizer config file saved in ./models_task1/tokenizer_config.json\n",
      "Special tokens file saved in ./models_task1/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./models_task1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Spoiler Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase/Passage Spoiler Generation using QA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2squadFormat(df):\n",
    "    df_fin = df[['uuid','targetTitle','postText',\"mergedParas\",\"tokPos\",\"spoiler\"]]\n",
    "    df_fin[\"asnwers\"] = df_fin.apply(lambda x: {'text':x['spoiler'], \"answer_start\":[x['tokPos'][0][0]]},1)\n",
    "    df_fin = df_fin.drop(columns=[\"tokPos\",\"spoiler\"])\n",
    "    df_fin.columns = [\"id\",\"title\",\"question\",\"context\",\"answers\"]\n",
    "    \n",
    "    return df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2e2c61ddea4c50877dd2d1a2cf21b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a420ea32a5af47559415e1b96a9e38e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b172658e71f43a481e59311312dbab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda6c3470da644efb000aca89e94602c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09630a5488a4727b1fc83f0c310f3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "squad_metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1274, 322)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on spoiler type you want to train, \"phrase\" can be replaced with \"passage\"\n",
    "spoiler_type = \"phrase\"\n",
    "train_df = df_train[df_train.tags==spoiler_type]\n",
    "val_df = df_valid[df_valid.tags==spoiler_type]\n",
    "\n",
    "len(train_df),len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164/3017848759.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fin[\"asnwers\"] = df_fin.apply(lambda x: {'text':x['spoiler'], \"answer_start\":[x['tokPos'][0][0]]},1)\n"
     ]
    }
   ],
   "source": [
    "train_df = convert2squadFormat(train_df)\n",
    "val_df = convert2squadFormat(val_df)\n",
    "\n",
    "train_data = Dataset.from_pandas(train_df.reset_index(drop=True), split=\"train\")\n",
    "val_data = Dataset.from_pandas(val_df.reset_index(drop=True), split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples):\n",
    "    questions = examples['question']\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, features, examples, predictOnly=False):\n",
    "    example_to_features = defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_spoilers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        spoilers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    spoilers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(spoilers) > 0:\n",
    "            best_answer = max(spoilers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_spoilers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_spoilers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "            \n",
    "    predicted_texts = [i['prediction_text'] for i in predicted_spoilers]\n",
    "    \n",
    "    if predictOnly:\n",
    "        return predicted_texts\n",
    "    \n",
    "    actual_spoilers_squad = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    actual_spoilers = [i['answers']['text'][0] for i in actual_spoilers_squad]    \n",
    "    \n",
    "    squad_metrics_eval = squad_metric.compute(predictions=predicted_spoilers, references=actual_spoilers_squad)\n",
    "    bleu_eval = bleu.compute(predictions=predicted_texts, references=actual_spoilers)\n",
    "    \n",
    "    return [squad_metrics_eval,bleu_eval],actual_spoilers,predicted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"csarron/bert-base-uncased-squad-v1\"\n",
    "\n",
    "model_name = \"deepset/minilm-uncased-squad2\"\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# Para Generation\n",
    "# config = dict(\n",
    "# max_length = 512,\n",
    "# stride = 128,\n",
    "# n_best = 15,\n",
    "# max_answer_length = 100,\n",
    "# batch_size = 8,\n",
    "# epochs = 20,\n",
    "# learning_rate = 1e-6,\n",
    "# model_name = model_name,\n",
    "# spoiler_type = \"passage\"\n",
    "# )\n",
    "\n",
    "# Phrase Generation\n",
    "config = dict(\n",
    "max_length = 512,\n",
    "stride = 128,\n",
    "n_best = 25,\n",
    "max_answer_length = 30,\n",
    "batch_size = 8,\n",
    "epochs = 10,\n",
    "learning_rate = 1e-6,\n",
    "model_name = model_name,\n",
    "spoiler_type = \"phrase\"\n",
    ")\n",
    "\n",
    "\n",
    "max_length = config[\"max_length\"]\n",
    "stride = config[\"stride\"]\n",
    "n_best = config[\"n_best\"]\n",
    "max_answer_length = config[\"max_answer_length\"]\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(config[\"model_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31a8da326424013bb52ccc9fe3bb621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b27b096494b4df79b023b8be8ead7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2618, 695)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_data.map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=train_data.column_names,\n",
    ")\n",
    "\n",
    "validation_dataset = val_data.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=val_data.column_names,\n",
    ")\n",
    "\n",
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "validation_set.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=config[\"batch_size\"],\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    validation_set, collate_fn=default_data_collator, batch_size=8 \n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "accelerator = Accelerator(fp16=True)\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "num_train_epochs = config[\"epochs\"]\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb6f6e069844f65be14c7010b5e2cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jmc3acnx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30011fcbc2d542df927cf12bde29824e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>bleu</td><td>▁▄▅▆▇▇▇▇▇████▇██████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>exact_match</td><td>▁▄▅▆█▇▇▇████████████</td></tr><tr><td>f1</td><td>▁▃▅▆▇▇▇▇▇███████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>766.12697</td></tr><tr><td>bleu</td><td>0.31384</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>exact_match</td><td>14.28571</td></tr><tr><td>f1</td><td>41.7913</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">eager-night-54</strong>: <a href=\"https://wandb.ai/rajasvi/clickbait_task_2/runs/jmc3acnx\" target=\"_blank\">https://wandb.ai/rajasvi/clickbait_task_2/runs/jmc3acnx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221204_221529-jmc3acnx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jmc3acnx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd3332910c440c09d0ac247f2bacbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668914631009103, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rvsharma/lign167/project/wandb/run-20221204_223709-3em35cq6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rajasvi/clickbait_task_2/runs/3em35cq6\" target=\"_blank\">silvery-serenity-55</a></strong> to <a href=\"https://wandb.ai/rajasvi/clickbait_task_2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cf8fe9c2b443b49ab907b3ee583c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecfb68bb31b4697a6480be6371abbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/322 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: [{'exact_match': 8.385093167701863, 'f1': 27.781328788305274}, {'bleu': 0.21098499339957522, 'precisions': [0.31653285756764793, 0.22517176764522173, 0.21264367816091953, 0.2061873487729001], 'brevity_penalty': 0.8923593123789505, 'length_ratio': 0.897757608115323, 'translation_length': 6726, 'reference_length': 7492}]\n",
      "Evaluation!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63ed01b2ee54c37b26008d111461dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a9db647190454b9b2f29a56ef88122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/322 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: [{'exact_match': 9.937888198757763, 'f1': 31.328850163220313}, {'bleu': 0.23682079211961637, 'precisions': [0.3136375288387593, 0.22820855614973262, 0.2138268156424581, 0.20552147239263804], 'brevity_penalty': 1.0, 'length_ratio': 1.0413774693005873, 'translation_length': 7802, 'reference_length': 7492}]\n",
      "Evaluation!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6667862e023e4986a9cfdb9a7db92269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5bf6dc94fb40a98ce0ae4485ce6869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/322 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: [{'exact_match': 11.490683229813664, 'f1': 34.68306952507956}, {'bleu': 0.25529958408122627, 'precisions': [0.3292465182716343, 0.2471840574328506, 0.23289083644799588, 0.22413329750067187], 'brevity_penalty': 1.0, 'length_ratio': 1.1213294180459157, 'translation_length': 8401, 'reference_length': 7492}]\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "wandb.init(\n",
    "  project=\"clickbait_task_2\", entity=\"rajasvi\",\n",
    "  config=config,\n",
    ")\n",
    "\n",
    "all_metrics = []\n",
    "max_bleu = 0.35\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        train_loss+=loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    accelerator.print(\"Evaluation!\")\n",
    "    \n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(validation_dataset)]\n",
    "    end_logits = end_logits[: len(validation_dataset)]\n",
    "\n",
    "    metrics,theoretical_texts,predicted_texts = compute_metrics(\n",
    "        start_logits, end_logits, validation_dataset, val_data\n",
    "    )\n",
    "    all_metrics.append(metrics)\n",
    "    wandb.log({\n",
    "        \"epoch\":epoch,\n",
    "        \"Train Loss\": train_loss,\n",
    "        \"exact_match\": metrics[0]['exact_match'],\n",
    "        \"f1\": metrics[0]['f1'],\n",
    "        \"bleu\": metrics[1]['bleu']        \n",
    "    })\n",
    "    if metrics[1]['bleu']>max_bleu:\n",
    "        model.save_pretrained(f\"./models_task2_{spoiler_type}/\")\n",
    "        max_bleu = metrics[1]['bleu']\n",
    "    print(f\"epoch {epoch}:\", metrics)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Model: Passage Spoiler Generation through Ranking Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rvsharma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7375819d3914e5f93a97c5117212609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f946eb512148bdbb463a247ad0e661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab6f06e759a40c9aa9fc1c9f5e5435e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84671251cec4cd8aaba3588fb8a5a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4c453d9c7540289d593850f5347374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from itertools import chain\n",
    "\n",
    "nltk.download('punkt')\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "squad_metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_para = df_valid[df_valid.tags==\"passage\"][[\"uuid\",\"targetTitle\",\"postText\",\"targetParagraphs\",\"spoiler\",\"tokPos\"]].reset_index(drop=True)\n",
    "df_para[\"targetParagraphsWithTitle\"] = df_para.apply(lambda x: [x['targetTitle']] + x['targetParagraphs'] if x['targetTitle']!=x['postText'] else x['targetParagraphs'],1)\n",
    "df_para[\"answers\"] = df_para.apply(lambda x: {'text':x['spoiler'], \"answer_start\":[x['tokPos'][0][0]]},1)\n",
    "df_para[\"sents\"] = df_para.targetParagraphsWithTitle.apply(lambda x: list(chain.from_iterable([sent_tokenize(y) for y in x])),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c17a84155b41979cf2485dc7e27d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/322 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"\n",
    "# name = \"cross-encoder/stsb-TinyBERT-L-4\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "best_paras = []\n",
    "model.eval()\n",
    "\n",
    "for ind, row in tqdm(df_para.iterrows(), total=len(df_para)):\n",
    "    paras = row['targetParagraphs']\n",
    "    query = [row['postText']]*len(paras)\n",
    "    features = tokenizer(query,paras,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         print([x for x in model(**features).logits])\n",
    "        scores = [x.item() for x in model(**features).logits]\n",
    "    scores = list(zip(scores,paras))\n",
    "    scores.sort(reverse=True)\n",
    "    best_paras.append([x[1] for x in scores[:5]])\n",
    "#     break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77853fc6c5134e60939e70bca02f5f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/322 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_sents = []\n",
    "\n",
    "for i in tqdm(range(len(best_paras))):\n",
    "    candidates = []\n",
    "    for p in best_paras[i][:1]:\n",
    "        candidates+=sent_tokenize(p)\n",
    "        \n",
    "    query  = [df_para.iloc[i][\"postText\"]]*len(candidates)\n",
    "    features = tokenizer(query, candidates,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = [x.item() for x in model(**features).logits]\n",
    "    scores = list(zip(scores, candidates))\n",
    "    scores.sort(reverse=True)\n",
    "    best_sents.append(scores[0][1])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "theoretical_texts_squad = [{\"id\": ex[\"uuid\"], \"answers\": ex[\"answers\"]} for ind,ex in df_para.iterrows()]\n",
    "predictions_squad = [{\"id\":df_para.iloc[i][\"uuid\"],\"prediction_text\":sent} for i,sent in enumerate(best_sents)]\n",
    "theoretical_texts = [i['answers']['text'][0] for ind,i in df_para.iterrows()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.12175138133006554,\n",
       " 'precisions': [0.22077136984645146,\n",
       "  0.10944772927412254,\n",
       "  0.09754818408286352,\n",
       "  0.09322381930184805],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.1039775760811532,\n",
       " 'translation_length': 8271,\n",
       " 'reference_length': 7492}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.compute(predictions=best_sents, references=theoretical_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 3.7267080745341614, 'f1': 19.911430180638625}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_metric.compute(predictions=predictions_squad, references=theoretical_texts_squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "76cf22aa59bf4331523856a689655e3c9f7e854c14a1856d53c952cbb33d1178"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
